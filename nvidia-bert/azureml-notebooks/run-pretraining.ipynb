{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import sys\n",
    "\n",
    "# AzureML libraries\n",
    "import azureml.core\n",
    "from azureml.core import Experiment, Workspace, Datastore, Run\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.container_registry import ContainerRegistry\n",
    "from azureml.core.runconfig import MpiConfiguration, RunConfiguration, DEFAULT_GPU_IMAGE\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consult microsoft-onnxruntime-example/nvidia-bert/README.md for instructions prior to running this notebook.\n",
    "\n",
    "# Create or retrieve Azure machine learning workspace\n",
    "# see https://docs.microsoft.com/en-us/python/api/overview/azure/ml/?view=azure-ml-py\n",
    "ws = Workspace.get(name=\"myworkspace\", subscription_id='<azure-subscription-id>', resource_group='myresourcegroup')\n",
    "\n",
    "# Print workspace attributes\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Workspace region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a datastore from blob storage containing training data.\n",
    "# Consult README.md for instructions downloading and uploading training data.\n",
    "ds = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                             datastore_name='<datastore-name>',\n",
    "                                             account_name='<storage-account-name>', \n",
    "                                             account_key='<storage-account-key>',\n",
    "                                             container_name='<storage-container-name>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print datastore attributes\n",
    "print('Datastore name: ' + ds.name, \n",
    "      'Container name: ' + ds.container_name, \n",
    "      'Datastore type: ' + ds.datastore_type, \n",
    "      'Workspace name: ' + ds.workspace.name, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GPU cluster\n",
    "gpu_cluster_name = \"ndv2scus\" \n",
    "try:\n",
    "    gpu_compute_target = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_ND40rs_v2', min_nodes=4, max_nodes=4)\n",
    "    gpu_compute_target = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
    "    gpu_compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment for phase 1\n",
    "experiment_name = 'nvbert-ort-pretraining-phase1'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_folder = '../workspace'\n",
    "\n",
    "# see README.md for instructions pushing Docker image with onnxruntime build\n",
    "image_name = 'bert-onnxruntime:latest'\n",
    "\n",
    "# credentials to registry containing above Docker image\n",
    "cr = ContainerRegistry()\n",
    "cr.address = '<registry-name>.azurecr.io'\n",
    "cr.username = '<registry-username>'\n",
    "cr.password = '<registry-password>'\n",
    "\n",
    "# set MPI configuration\n",
    "# set processes per node to be equal to GPU count on SKU.\n",
    "mpi = MpiConfiguration()\n",
    "mpi.process_count_per_node = 8\n",
    "\n",
    "import uuid\n",
    "output_id = uuid.uuid1().hex\n",
    "\n",
    "# Define training estimator for phase 1\n",
    "# Consult https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-ml-models\n",
    "# Fill in blob path to phase 1 training data in argument below\n",
    "estimator_ph1 = Estimator(source_directory=project_folder,\n",
    "\n",
    "                    # Compute configuration\n",
    "                    compute_target = gpu_compute_target,\n",
    "                    node_count=4,\n",
    "                    process_count_per_node=1,  # separate MPI jobs\n",
    "                    distributed_training = mpi,\n",
    "                    use_gpu = True,\n",
    "                    \n",
    "                    # supply Docker image\n",
    "                    use_docker = True,\n",
    "                    custom_docker_image = image_name,\n",
    "                    image_registry_details = cr,\n",
    "                    user_managed = True,\n",
    "                    \n",
    "                    # Training script parameters\n",
    "                    script_params = {\n",
    "                        \"--config_file\": \"bert_config.json\",\n",
    "                        '--input_dir' : ds.path('<blob-path-to-phase1-training-data>').as_mount(), \n",
    "                        '--output_dir': ds.path(f'output/{experiment_name}/{output_id}/').as_mount(),\n",
    "                        '--bert_model' : 'bert-large-uncased',\n",
    "                        '--train_batch_size' : 4096,\n",
    "                        '--max_seq_length': 128,\n",
    "                        '--max_predictions_per_seq': 20,\n",
    "                        '--max_steps' : 7038,\n",
    "                        '--warmup_proportion' : '0.2843',\n",
    "                        '--num_steps_per_checkpoint' : 200,\n",
    "                        '--learning_rate' : '6e-3',\n",
    "                        '--seed': 42,\n",
    "                        '--fp16' : '',\n",
    "                        '--gradient_accumulation_steps' : 32,\n",
    "                        '--allreduce_post_accumulation' : '',\n",
    "                        '--allreduce_post_accumulation_fp16' : '',\n",
    "                        '--do_train' : '',\n",
    "                        '--use_ib' : '', # pass if infiniband available on SKU\n",
    "                        '--gpu_memory_limit_gb' : 32 # set to per GPU memory in GB (check SKU)\n",
    "                    },\n",
    "                    \n",
    "                    entry_script = 'run_pretraining_ort.py',\n",
    "                    inputs = [ds.path('').as_mount()]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit phase 1 (check logs from Outputs + logs tab of corresponding link)\n",
    "run = experiment.submit(estimator_ph1)\n",
    "RunDetails(run).show()\n",
    "print(run.get_portal_url())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experiment for phase 2\n",
    "experiment_name = 'nvbert-ort-pretraining-phase2'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define training estimator for phase 2\n",
    "# Fill in blob path to phase 1 training data as well as phase 1 checkpoint in arguments below\n",
    "estimator_ph2 = Estimator(source_directory=project_folder,\n",
    "\n",
    "                    # Compute configuration\n",
    "                    compute_target = gpu_compute_target,\n",
    "                    node_count=4, \n",
    "                    process_count_per_node=1, # separate MPI jobs\n",
    "                    distributed_training = mpi,\n",
    "                    use_gpu = True,\n",
    "                    \n",
    "                    #Docker image\n",
    "                    use_docker = True,\n",
    "                    custom_docker_image = image_name,\n",
    "                    image_registry_details = cr,\n",
    "                    user_managed = True,\n",
    "                    \n",
    "                    # Training script parameters\n",
    "                    script_params = {\n",
    "                        # Required Params\n",
    "                        \"--config_file\": \"bert_config.json\",\n",
    "                        '--input_dir' : ds.path('<blob-path-to-phase2-training-data>').as_mount(), \n",
    "                        '--output_dir': ds.path(f'output/{experiment_name}/{output_id}/').as_mount(),\n",
    "                        '--bert_model' : 'bert-large-uncased',\n",
    "                        '--train_batch_size' : 4096,\n",
    "                        '--max_seq_length': 512,\n",
    "                        '--max_predictions_per_seq': 80,\n",
    "                        '--max_steps' : 1563,\n",
    "                        '--warmup_proportion' : '0.128',\n",
    "                        '--num_steps_per_checkpoint' : 200,\n",
    "                        '--learning_rate' : '4e-3',\n",
    "                        '--seed': 42,\n",
    "                        '--fp16' : '',\n",
    "                        '--gradient_accumulation_steps' : 256,\n",
    "                        '--allreduce_post_accumulation' : '',\n",
    "                        '--allreduce_post_accumulation_fp16' : '',\n",
    "                        '--do_train' : '',\n",
    "                        '--phase2' : '',\n",
    "                        '--resume_from_checkpoint' : '',\n",
    "                        '--phase1_end_step' : '7038',\n",
    "                        '--init_checkpoint' : ds.path('<path-to-checkpoint-from-phase-1>'),\n",
    "                        '--use_ib' : '', # pass if infiniband available on SKU\n",
    "                        '--gpu_memory_limit_gb' : 32 # set to per GPU memory in GB (check SKU)\n",
    "                    },\n",
    "                    \n",
    "                    entry_script='run_pretraining_ort.py',\n",
    "                    inputs=[ds.path('').as_mount()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit phase 2 run (check logs from Outputs + logs tab of corresponding link)\n",
    "run = experiment.submit(estimator_ph2)\n",
    "RunDetails(run).show()\n",
    "print(run.get_portal_url())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bitcc8d29fdaf404ed89af66bb734076d9e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}